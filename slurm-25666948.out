2023-09-17 10:38:24.729319: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-17 10:38:25.923426: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Number of parameters: 454944
Epoch 1
Training Loss: 0.8614231845649569
Selected bands: [  8  13  16  24  28  31  38  44  62  66  68  93 108 112 114 122 127 147
 155 167 177 190 191 192 194]
penalty: 0.33689969778060913

Epoch 2
Training Loss: 0.00017944265995337803
Selected bands: [  3   9  11  12  47  48  58  70 100 104 112 116 119 121 122 131 132 141
 155 156 164 167 169 172 193]
penalty: 0.0008045219583436847

Epoch 3
Training Loss: 2.5676609461789233e-05
Selected bands: [  5   8   9  14  28  45  55  58  64  67  74  76  91 100 108 109 116 133
 136 167 169 171 181 184 190]
penalty: 0.0005481971893459558

Epoch 4
Training Loss: 6.723878710926999e-06
Selected bands: [  5   9  14  28  33  55  64  70  76  88  93 100 108 109 116 133 136 158
 167 169 171 183 190 194 195]
penalty: 1.3885553471482126e-06

Epoch 5
Training Loss: 4.916592315005428e-06
Selected bands: [  5   9  14  28  50  58  61  64  70  75  76 100 109 111 116 122 133 136
 158 159 183 184 185 187 190]
penalty: 7.799145151921039e-08

Epoch 6
Training Loss: 3.094156743775309e-06
Selected bands: [  5   9  14  28  50  64  67  70  76  90  91 100 109 111 116 119 122 133
 136 138 153 155 158 167 190]
penalty: 5.876782172009598e-09

Epoch 7
Training Loss: 1.619336014642161e-06
Selected bands: [  5   9  14  28  64  70  74  76  93  99 100 109 111 116 117 119 133 136
 138 158 167 179 185 188 190]
penalty: 7.798502121847406e-11

Epoch 8
Training Loss: 1.8432661504442546e-06
Selected bands: [  5   9  14  28  67  70  74  76  93  99 100 109 111 116 117 133 136 138
 155 158 167 179 185 188 190]
penalty: 1.179433612406855e-12

Epoch 9
Training Loss: 1.365495428689038e-06
Selected bands: [  5   9  14  28  47  67  70  76  83  93  99 100 109 111 116 133 136 138
 155 167 179 185 187 188 190]
penalty: 7.274484387142442e-18

Epoch 10
Training Loss: 2.542447900206927e-06
Selected bands: [  5   9  14  28  47  70  76  83  93  99 100 109 111 116 133 136 138 155
 158 167 179 185 187 188 190]
penalty: 3.23140777061321e-23

Epoch 11
Training Loss: 1.2957346341646083e-07
Selected bands: [  5   9  14  28  47  70  76  83  93  99 100 109 111 116 133 136 138 155
 158 167 179 185 187 188 190]
penalty: 4.789876604542931e-18

Epoch 12
Training Loss: 1.4809518631596321e-06
Selected bands: [  5   9  14  28  47  70  76  83  93  99 100 109 111 116 133 136 138 155
 158 167 179 185 187 188 190]
penalty: 2.249333949541487e-26

Epoch 13
Training Loss: 1.4728100907640609e-06
Selected bands: [  5   9  14  28  47  70  76  83  93  99 100 109 111 116 133 136 138 155
 158 167 179 185 187 188 190]
penalty: 1.5003434823251346e-28

Epoch 14
Training Loss: 2.7270029559198982e-06
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 111 116 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 9.215326132399414e-36

Epoch 15
Training Loss: 5.434691033940646e-07
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 111 116 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 16
Training Loss: 1.2731635996813287e-06
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 111 116 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 17
Training Loss: 2.1960960428057422e-07
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 111 116 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 18
Training Loss: 3.758771007404844e-09
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 111 116 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 19
Training Loss: 2.938911308016813e-06
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 111 116 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 20
Training Loss: 1.0603968704656484e-09
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 116 119 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 21
Training Loss: 7.655475801275531e-07
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 116 119 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 22
Training Loss: 1.4556212731832345e-06
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 116 119 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 23
Training Loss: 4.879423549075626e-07
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 116 119 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 24
Training Loss: 1.8837885555023782e-06
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 116 119 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 25
Training Loss: 9.757050227619189e-07
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 116 119 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 26
Training Loss: 1.990731374764447e-06
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 116 119 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 27
Training Loss: 2.439262995966311e-06
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 116 119 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 28
Training Loss: 1.225595015536166e-06
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 116 119 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 29
Training Loss: 1.951410365995074e-06
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 116 119 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 30
Training Loss: 1.9510551778563852e-06
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 116 119 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 31
Training Loss: 4.878529320653353e-07
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 116 119 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 32
Training Loss: 1.4619793409704524e-06
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 116 119 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 33
Training Loss: 4.878528932912994e-07
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 116 119 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 34
Training Loss: 2.439262726265639e-06
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 116 119 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 35
Training Loss: 3.9792757557744155e-13
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 116 119 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 36
Training Loss: 4.878528458087554e-07
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 116 119 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 37
Training Loss: 1.4635577571883844e-06
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 116 119 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 38
Training Loss: 1.9514102070417866e-06
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 116 119 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 39
Training Loss: 9.757057803727856e-07
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 116 119 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Epoch 40
Training Loss: 1.4635578591714142e-06
Selected bands: [  9  14  28  47  70  76  83  93  99 100 109 116 119 133 136 138 155 158
 166 167 179 187 188 189 190]
penalty: 0.0

Dataset: IP. The learning rate is: 0.001. hidden dim is: [256, 256]. The result is: {'knn': {'ca': array([[15.33, 68.36, 58.43, 29.09, 83.15, 92.79, 68.73, 97.9 , 10.65,
        70.44, 75.95, 42.4 , 94.98, 93.72, 20.5 , 83.78],
       [13.96,  4.22,  3.03,  8.51,  4.22,  2.25, 24.33,  3.45, 13.67,
         3.81,  2.81,  4.57,  2.28,  1.14,  6.43,  1.78]]), 'oa': array([62.89,  1.99]), 'aa': array([72.62,  0.59]), 'kappa': array([68.64,  0.65])}, 'svm': {'ca': array([[69.4 , 75.86, 69.33, 63.59, 87.72, 92.68, 82.39, 97.26, 44.11,
        68.65, 73.58, 73.22, 97.76, 89.39, 57.29, 89.92],
       [10.49,  2.51,  2.86,  9.12,  3.42,  1.55, 27.89,  1.95, 34.11,
         2.94,  0.97,  3.46,  2.42,  2.49,  6.12,  6.36]]), 'oa': array([77.01,  2.97]), 'aa': array([77.86,  0.45]), 'kappa': array([74.8 ,  0.52])}}.
